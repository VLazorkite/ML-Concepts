{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network #\n",
    "## Python Tutorial ##\n",
    "\n",
    "Neural networks are computational models inspired by the human brain's structure and function, capable of learning and making predictions from complex data.\n",
    "\n",
    "This looks something like this\n",
    "\n",
    "![SimpleNN](https://www.researchgate.net/publication/337469702/figure/fig1/AS:828416181932032@1574521217879/Simple-neural-network-diagram-http-cs231ngithubio-neuralnetworks-1-The-nodes-are.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks process input data through layers of neurons, where each neuron applies transformations to the data using activation functions and learns to make predictions by adjusting its parameters during training, aiming to minimize prediction errors.\n",
    "\n",
    "<i>Note: There are as many hidden layers and neurons as you want and more does not necessarily imply better</i>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries ####\n",
    "\n",
    "Here are pieces of other people's code that will help as understand what we're doing along the way\n",
    "\n",
    "To install these click terminal on whichever IDE you're using and use pip to instal eg: <code>pip install matplotlib.pyplot</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do It ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the MNIST dataset which is a dataset for training simple neural networks on handwritten digits\n",
    "\n",
    "We will train and run our model on subsets of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handwritting_data_raw = pd.read_csv(\"data/Handwritting_dataset/train.csv\")\n",
    "handwritting_data_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents the a handwritten digit in a 28 by 28 pixel grid and the value brigtness of each pixel.\n",
    "\n",
    "This is how a digit would look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11b708850>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert the data to numpy arrays\n",
    "hData = np.array(handwritting_data_raw).T\n",
    "m, n = hData.T.shape\n",
    "\n",
    "exampleMatrix = hData.T[3][1:].reshape(28, 28)\n",
    "\n",
    "plt.imshow(exampleMatrix, cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation ##\n",
    "This is the passing of data from the input layer to the output layer through the hidden layers. Each neuron after the input to output layer has 2 components called the weights and bias. <b>Weights</b> determine the strength of influence that one neuron has with the next neuron it transmits information to. <b>Biases</b> are the initial value that a weight can possess even if all parameters are 0.\n",
    "\n",
    "The process of information transmission is done through matrix multiplication of the input and the weights and addition of biases of each neutron much resembling a linear function. The result of this is then applied through an activation function. \n",
    "\n",
    "The <b>activation function</b> is a function that changes the output from being linear which gives the neural network all its magic. This is like painting a picture but now from only using straight lines we can introduce curves and contrasts. The activation then refers to the final output value.\n",
    "\n",
    "![Yo](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*U3QZ_Yn4fcjbdUkIwHJ90w.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math ###\n",
    "<i>A basic understanding of matrices and matrix multiplication is needed<i>\n",
    "\n",
    "So a neuron receives input and applies a linear transformation to it and apply an activation function to it.\n",
    "\n",
    "$$ r = wx + b $$\n",
    "$$ a = f(z) $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$ w(weights) \\in \\mathbb{R}^{m\\times1} ,\\hspace{5mm} x(inputs) \\in \\mathbb{R}^{1\\times m} ,\\hspace{5mm} b(bias), \\hspace{1.5mm} z(linear \\hspace{1.5mm} result), \\hspace{2mm} a(activation) \\in  \\mathbb{R}^1$$\n",
    "\n",
    "and f is the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can scale this up to a whole layer of neurons so where for a neuron in layer i:\n",
    "\n",
    "$$ w^{[i]} = \\begin{bmatrix}w^{[i]}_1, w^{[i]}_2, & \\cdots &, w^{[i]}_n \\end{bmatrix}, \\hspace{5mm} x = \\begin{bmatrix}x^{[i]}_1, x^{[i]}_2, & \\cdots &, x^{[i]}_n \\end{bmatrix}^T \\hspace{3mm} and \\hspace{3mm} b^{[i]} = b^{[i]} $$\n",
    "\n",
    "The operations in the layer can be represented with the equations:\n",
    "\n",
    "$$ Z = WX + B$$\n",
    "$$ A = f(R) $$\n",
    "\n",
    "where\n",
    "$$ W^{[i]} = \\begin{bmatrix}w^{[i]}_{11} & w^{[i]}_{12} & \\cdots & w^{[i]}_{1n}\\\\ w^{[i]}_{21} & \\ddots& \\cdots & \\vdots \\\\ \\vdots & \\vdots& \\ddots & \\vdots\\\\ w^{[i]}_{m1} & \\cdots & \\cdots & w^{[i]}_{mn}\\\\ \\end{bmatrix}, \\hspace{5mm} X^{[i]} = \\begin{bmatrix}x^{[i]}_{11} & x^{[i]}_{12} & \\cdots & x^{[i]}_{1n}\\\\ x^{[i]}_{21} & \\ddots& \\cdots & \\vdots \\\\ \\vdots & \\vdots& \\ddots & \\vdots\\\\ x^{[i]}_{m1} & \\cdots & \\cdots & x^{[i]}_{mn}\\\\ \\end{bmatrix}^T \\hspace{3mm} and \\hspace{3mm} B^{[i]} = \\begin{bmatrix}b^{[i]}_1 \\\\ b^{[i]}_2 \\\\ \\vdots \\\\ b^{[i]}_m\\end{bmatrix} $$ \n",
    "\n",
    "This works because matrices can be layered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for our neural network we will have 784 neurons in the input layers for the 28 by 28 pixel grid and I chose 1 hidden layer with 16 neurons and lastly the output layer will have 10, corresponding to the 10 digits it could be.\n",
    "\n",
    "<i>Note: The input layer does not have any weights and biases because it introduces primary data.</i>\n",
    "\n",
    "We first want to normalise the pixel data as each value is given from 0 to 255 which is computationally complex whereas working from 0 to 1 is much better.\n",
    "\n",
    "When initialising The values for weights and biases can just be randomised as the values are updated after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronLayer:\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        # creates a randomised matrix\n",
    "        self.weights = np.random.rand(outputSize, inputSize) - 0.5\n",
    "        self.bias = np.random.rand(outputSize, 1) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data\n",
    "Y = hData[0]\n",
    "L0 = hData[1:] / 255\n",
    "L1 = NeuronLayer(784, 16)\n",
    "L2 = NeuronLayer(16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our activation function we have some guidelines to choosing one that works.\n",
    "<ul>\n",
    "<li><b>Non-linear</b> function, as mentioned earlier to have complexity</li>\n",
    "<li>It has to be <b>differentiable</b> this is to enable optimization which will be seen later when training</li>\n",
    "<li>It should have <b>appropriate range</b>. For classifying multiple classes some functions produce probability functions and for binary classification functions with a smaller range work well</li>\n",
    "<li><b>Computational complexity</b> should be low and effecient to evaluate especially as training large models takes significant resources</li>\n",
    "</ul>\n",
    "\n",
    "Common activation functions are:\n",
    "\n",
    "\n",
    "![activationfunctions](https://lh4.googleusercontent.com/hTeaMXYrsBlpKrGvRCvSX8maYuU4Zhd9-6B_Z3QjnnpE02MhfFK8IHgrDsX9U9SoSw9MIJFQbQyR64PHqNjGfMa8LgUctX5ht0Z21NxqJ-AAd5bU30mFGaTzNhiNuiwO2OVvpfYYFAonf3k8wQTqwGA)\n",
    "\n",
    "The functions we use are : [ReLU]('https://www.nomidl.com/wp-content/uploads/2022/04/image-10.png') which returns values between 0 and 1(We normalised the data) for the hidden layer and [Softmax]('https://siegel.work/blog/ActivationFunctions/img/softmax.png') which returns exaggarated probability values for the output layer\n",
    "\n",
    "$$ ReLU(Z) = max (0,Z) $$\n",
    "$$softmax(Z) = \\frac{e^{z_i}}{\\sum_{j=1}^{N} e^{z_j}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z,0)\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly lets put all these steps together to create a function called foward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(input,hiddenlayer,output):\n",
    "    Z1 = hiddenlayer.weights.dot(input) + hiddenlayer.bias\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = output.weights.dot(A1) + output.bias\n",
    "    A2 = softmax(Z2)\n",
    "    # print(hiddenlayer['weights'])\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flow ####\n",
    "\n",
    "$$ \\rightarrow Z^{[1]} = X^{[0]}W^{[1]} + B^{[1]} $$ \n",
    "$$ \\rightarrow A^{[1]} = f_{ReLU}(Z^{[1]}) $$\n",
    "$$ \\rightarrow Z^{[2]} = A^{[1]T}\\hspace{1mm}W^{[2]} + B^{[2]} $$\n",
    "$$ \\rightarrow A^{[2]} = g_{softmax}(Z^{[2]}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation ##\n",
    "So how does the neural network learn?\n",
    "\n",
    "Great question! This is called backpropagation which is the key algorithm for training our model. As you can see we randomised our weights and biases when initialising, this is because adjusting each weight and bias manually would take ages.\n",
    "\n",
    "We train our model by letting our model guess and then telling it how far it was from the correct answer to update its parameters. Your goal is to minimize the distance to the correct answer. Think of training a neural network like navigating through a maze to reach a treasure. The cost function is like a guide telling you how far you are from the treasure after each step you take. Your goal is to minimize this distance to get as close to the treasure as possible.\n",
    "\n",
    "We call the distance to treasure the <b>Loss function</b>. We then want to change the weights and biases in each layer to minimize this. \n",
    "\n",
    "![bpflow](https://qph.cf2.quoracdn.net/main-qimg-aa0cc96279f525b44bbde531ec1e9a9c)\n",
    "\n",
    "\n",
    "We use a method called <b>gradient descent</b>. This is subtracting a value from the parameters relative to the gradient of the loss function to approach the minimum value. It's like taking small steps down to approach the minima. The length of the step we chose to go down by is the <b>learning rate</b>.\n",
    "\n",
    "\n",
    "![gradientdesc](https://media.licdn.com/dms/image/D4D12AQElGrpg2NiisQ/article-cover_image-shrink_600_2000/0/1707688084849?e=2147483647&v=beta&t=iBiIxGUrle6a1mlTadU-0vWvyVjCxW7DBa5qXqK_Qa4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math ###\n",
    "<i>The tricky bit. It's long and gruesome and you can colapse it if you'd like, but it is the derivation of possibly the coolest code we have ever made</i>\n",
    "\n",
    "A decent understanding of calculus is needed but the steps we take are:\n",
    "<ul>\n",
    "<li>Chose a loss function</li>\n",
    "<li>Minimize it with respect to weights and biases</li>\n",
    "<li>Gradient descent. These are the steps that vary the weights and biases to most effeciently decrease the loss function</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivations ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to chose a loss function. To keep it simple we will use a binary cross entropy loss function which simplifies well when minimizing.\n",
    "$$ L = -\\frac{1}{m} \\sum_{i=1}^{m} \\hspace{2mm} y_i\\ln{\\hat{y_i}} + (1-y_i)ln{(1 - \\hat{y_i})} $$\n",
    "$$ L = \\text{loss} \\hspace{5mm} m = \\text{number of output neurons} \\hspace{5mm} y_i = \\text{correct probability} \\hspace{5mm} \\hat{y_i} = \\text{predicted probability} $$\n",
    "\n",
    "We can represent this with our matrix notation from before:\n",
    "\n",
    "$$ L = -\\frac{1}{m} \\sum_{i=1}^{m} \\hspace{2mm} y_i\\ln{a^{[2]}_i} + (1-y_i)ln{(1 - a^{[2]}_i)} $$\n",
    "\n",
    "<!-- $$ L = \\frac{1}{m} \\sum_{i=1}^{m} \\hspace{2mm} Y\\ln{A^{[2]}} + (1-Y)ln{(1 - A^{[2]})} $$ -->\n",
    "\n",
    "As\n",
    "$$ \\hat{y} = a^{[2]} \\hspace{5mm} $$\n",
    "\n",
    "\n",
    "Now we want want this loss to decrease most for the smallest change in the weights and biases. As usual with minimization problems we need calculus. \n",
    "Differenciate the loss with respect to the predictions for a single output neuron\n",
    "\n",
    "$$ \\frac{dL}{da^{[2]}_i}= -\\frac{y_i}{a^{[2]}_i} + \\frac{1-y_i}{1-a^{[2]}_i}$$\n",
    "We then want to then find the the minimum with respect to the weight and biases. \n",
    "\n",
    "To get the minimum of the loss with respect to the weights we need to calculate the partial deriavtive. Which we can use the chain rule:\n",
    "\n",
    "$$ \\frac{dL}{dz^{[2]}_i} =  \\frac{\\partial L}{\\partial a^{[2]}_i}  \\frac{\\partial a^{[2]}_i}{\\partial z^{[2]}_i} $$\n",
    "<!-- $$ \\frac{dL}{dZ} =  \\frac{\\partial L}{\\partial A^{[2]}}  \\frac{\\partial A^{[2]}}{\\partial Z^{[2]}}, \\hspace{5mm} \\frac{\\partial L}{\\partial W^{[2]}} = \\frac{\\partial L}{\\partial Z^{[2]}} \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} \\hspace{5mm} and \\hspace{5mm} \\frac{\\partial L}{\\partial B^{[2]}} = \\frac{\\partial L^{[2]}}{\\partial Z^{[2]}}  \\frac{\\partial Z^{[2]}}{\\partial B^{[2]}}$$ -->\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since \n",
    "$$ a^{[2]}_i = g(z^{[2]}_i)$$\n",
    "\n",
    "Therefore:\n",
    "$$ \\frac{\\partial a^{[2]}_i}{\\partial z^{[2]}_i} = g'(z^{[2]}_i) $$\n",
    "\n",
    "G is our softmax function who's formula is:\n",
    "$$g(Z) = \\frac{e^{z_i}}{\\sum_{j=1}^{N} e^{z_j}} $$\n",
    "\n",
    "Now we get the derivative using the quotient rule:\n",
    "$$ g'(Z) = \\frac{e^{z_i}(\\sum_{j=1}^{N} e^{z_j}) - e^{z_i}(e^{z_i})}{(\\sum_{j=1}^{N} e^{z_j})^2} $$\n",
    "\n",
    "We factorize:\n",
    "\n",
    "$$ g'(Z) = (\\frac{e^{z_i}}{\\sum_{j=1}^{N} e^{z_j}}) (\\frac{\\sum_{j=1}^{N} e^{z_j}-e^{z_i}} {\\sum_{j=1}^{N} e^{z_j}}) $$\n",
    "\n",
    "We can split the latter part further:\n",
    "\n",
    "$$g'(Z) = (\\frac{e^{z_i}}{\\sum_{j=1}^{N} e^{z_j}}) (1 -\\frac{e^{z_i}} {\\sum_{j=1}^{N} e^{z_j}})$$\n",
    "\n",
    "Now substitute g(Z) where you can:\n",
    "\n",
    "$$ g'(Z) = g(Z) (1 - g(Z)) = A (1 - A)$$\n",
    "\n",
    "It cleans up nicely\n",
    "\n",
    "Now we substitute that in to find dl/dz we get\n",
    "$$ \\frac{\\partial L}{\\partial z^{[2]}_i} =  (-\\frac{y_i}{a^{[2]}_i} + \\frac{1-y_i}{1-a^{[2]}_i}) \\times a^{[2]}_i(1-a^{[2]}_i)$$\n",
    "\n",
    "This simplifies to give us our first computed equation:\n",
    "$$ \\frac{dL}{dz^{[2]}_i} = a^{[2]}_i - y_i $$ \n",
    "\n",
    "We can then scale this up for the whole layer(without the summation):\n",
    "$$ \\frac{dL}{dZ^{[2]}} = \\frac{1}{m}(A^{[2]} - Y) \\tag{1}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get those equations with respect to w and b we multiply that with dz/dw and dz/db.\n",
    "Remember:\n",
    "$$ Z^{[2]} =  W^{[2]}A^{[1]} + B^{[2]} $$\n",
    "\n",
    "so:\n",
    "$$ \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} = A^{[1]T},  \\hspace{5mm} \\frac{\\partial Z^{[2]}}{\\partial B^{[2]}} = 1 $$\n",
    "\n",
    "Combining to get:\n",
    "$$ \\frac{\\partial L}{\\partial W^{[2]}} = dZ \\times A^{[1]T} \\tag{2} $$\n",
    "\n",
    "Notice in our equations we don't have the summation, this is beacuse the for the weights in the previous neurons are matrices. However since the bias are singular values we reintroduce the sum:\n",
    "$$  \\frac{\\partial L}{\\partial B^{[2]}} = \\sum{dZ^{[2]}} \\tag{3}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we continue the chain rule to get the values for all previous weights and biases in the same way\n",
    "\n",
    "$$ \\frac{\\partial Z^{[2]}}{\\partial A^{[1]}} = W^{[2]T},\\hspace{5mm}\\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} = f^\\prime(Z^{[1]}) \\text{ (1 or 0)}, \\hspace{5mm}\\frac{\\partial Z^{[1]}}{\\partial W^{[1]}} = X, \\hspace{5mm}\\frac{\\partial Z^{[1]}}{\\partial B^{[1]}} = 1 $$\n",
    "\n",
    "Also for any derivative of the loss will be represented in short hand\n",
    "$$ \\frac{\\partial L}{\\partial A^{[1]}}  =  dZ^{[2]}\\hspace{1mm}W^{[2]T} \\tag{4}  $$\n",
    "$$ \\frac{\\partial L}{\\partial Z^{[1]}}  =  dA^{[1]} \\times f^\\prime(Z^{[1]}) \\tag{5} $$\n",
    "$$ \\frac{\\partial L}{\\partial W^{[1]}}  =  dZ^{[1]}\\hspace{1mm}X^{[0]T} \\tag{6}  $$\n",
    "$$ \\frac{\\partial L}{\\partial B^{[1]}}  =   \\sum{dZ^{[1]}} \\tag{7}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final equations ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatives\n",
    "$$ dZ^{[2]} = \\frac{A^{[2]} - Y}{m} $$\n",
    "$$ dW^{[2]} = dZ^{[2]}A^{[2]T} $$\n",
    "$$ dB^{[2]} = \\sum{dZ^{[2]}}$$\n",
    "$$ dA^{[1]} = W^{[2]T}dZ^{[2]} $$\n",
    "$$ dZ^{[1]} = dA^{[1]} \\times f^{\\prime} (Z^{[1]})$$\n",
    "$$ dW^{[1]} = dZ^{[1]} A^{[0]T}$$\n",
    "$$ dB^{[1]} = \\sum{dZ^{[1]}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement this in code. We also have to encode Y such that it gives a value for 1 on the correct digit and 0 for everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "\n",
    "def dReLU(Z):\n",
    "    # If true multiplie by 1 and 0 otherwise\n",
    "    return Z > 0\n",
    "\n",
    "\n",
    "def gradients(Z1, A1, A2, W2, X, Y):\n",
    "    hotY = one_hot(Y)\n",
    "    dZ2 = (1/m) * (A2 - hotY)\n",
    "    dW2 = dZ2.dot(A1.T)\n",
    "    dB2 = np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * dReLU(Z1)\n",
    "    dW1 = dZ1.dot(X.T)\n",
    "    dB1 = np.sum(dZ1)\n",
    "    return dW1, dB1, dW2, dB2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes\n",
    "$$ W^{[2]} := W^{[2]} - \\alpha dW^{[2]}$$\n",
    "$$ B^{[2]} := B^{[2]} - \\alpha dB^{[2]}$$\n",
    "$$ W^{[1]} := W^{[1]} - \\alpha dW^{[1]}$$\n",
    "$$ B^{[1]} := B^{[1]} - \\alpha dB^{[1]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(L1, L2, dW1, dB1, dW2, dB2, alpha):\n",
    "    L1.weights = L1.weights - alpha * dW1\n",
    "    L1.bias = L1.bias - alpha * dB1\n",
    "    L2.weights = L2.weights - alpha * dW2\n",
    "    L2.bias = L2.bias - alpha * dB2\n",
    "    return L1, L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now combine into a function called back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backPropagation(Y, L0, L1, L2, Z1, A1, A2, alpha):\n",
    "    dW1, dB1, dW2, dB2 = gradients(Z1, A1, A2, L2.weights, L0, Y)\n",
    "    L1, L2 = gradientDescent(\n",
    "        L1,\n",
    "        L2,\n",
    "        dW1,\n",
    "        dB1,\n",
    "        dW2,\n",
    "        dB2,\n",
    "        alpha,\n",
    "    )\n",
    "    return L1, L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ##\n",
    "Now let's prepare to train and test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    # print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(Y, L0, L1, L2, alpha, iterations):\n",
    "\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forwardPropagation(L0, L1, L2)\n",
    "        L1, L2 = backPropagation(Y, L0, L1, L2, Z1, A1, A2, alpha)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            predictions = get_predictions(A2)\n",
    "            accuracy = get_accuracy(predictions, Y)\n",
    "            \n",
    "            print( f\"Iteration:\", i, \"Accuracy:\", round(accuracy, 6) )\n",
    "    return L1, L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Accuracy: 0.043786\n",
      "Iteration: 10 Accuracy: 0.210643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 20 Accuracy: 0.358548\n",
      "Iteration: 30 Accuracy: 0.458024\n",
      "Iteration: 40 Accuracy: 0.527976\n",
      "Iteration: 50 Accuracy: 0.580476\n",
      "Iteration: 60 Accuracy: 0.621476\n",
      "Iteration: 70 Accuracy: 0.6525\n",
      "Iteration: 80 Accuracy: 0.677429\n",
      "Iteration: 90 Accuracy: 0.697071\n",
      "Iteration: 100 Accuracy: 0.71319\n",
      "Iteration: 110 Accuracy: 0.728286\n",
      "Iteration: 120 Accuracy: 0.740548\n",
      "Iteration: 130 Accuracy: 0.75131\n",
      "Iteration: 140 Accuracy: 0.759929\n",
      "Iteration: 150 Accuracy: 0.769667\n",
      "Iteration: 160 Accuracy: 0.777071\n",
      "Iteration: 170 Accuracy: 0.784452\n",
      "Iteration: 180 Accuracy: 0.791119\n",
      "Iteration: 190 Accuracy: 0.796929\n",
      "Iteration: 200 Accuracy: 0.802405\n",
      "Iteration: 210 Accuracy: 0.807262\n",
      "Iteration: 220 Accuracy: 0.81169\n",
      "Iteration: 230 Accuracy: 0.815833\n",
      "Iteration: 240 Accuracy: 0.819286\n",
      "Iteration: 250 Accuracy: 0.823167\n",
      "Iteration: 260 Accuracy: 0.826048\n",
      "Iteration: 270 Accuracy: 0.829143\n",
      "Iteration: 280 Accuracy: 0.832024\n",
      "Iteration: 290 Accuracy: 0.834762\n",
      "Iteration: 300 Accuracy: 0.837214\n",
      "Iteration: 310 Accuracy: 0.839762\n",
      "Iteration: 320 Accuracy: 0.841714\n",
      "Iteration: 330 Accuracy: 0.843881\n",
      "Iteration: 340 Accuracy: 0.845667\n",
      "Iteration: 350 Accuracy: 0.84719\n",
      "Iteration: 360 Accuracy: 0.848857\n",
      "Iteration: 370 Accuracy: 0.850238\n",
      "Iteration: 380 Accuracy: 0.851738\n",
      "Iteration: 390 Accuracy: 0.85319\n",
      "Iteration: 400 Accuracy: 0.854595\n",
      "Iteration: 410 Accuracy: 0.856048\n",
      "Iteration: 420 Accuracy: 0.857405\n",
      "Iteration: 430 Accuracy: 0.8585\n",
      "Iteration: 440 Accuracy: 0.859929\n",
      "Iteration: 450 Accuracy: 0.86131\n",
      "Iteration: 460 Accuracy: 0.86219\n",
      "Iteration: 470 Accuracy: 0.863405\n",
      "Iteration: 480 Accuracy: 0.864571\n",
      "Iteration: 490 Accuracy: 0.865714\n",
      "Iteration: 500 Accuracy: 0.866524\n",
      "Iteration: 510 Accuracy: 0.867262\n",
      "Iteration: 520 Accuracy: 0.868048\n",
      "Iteration: 530 Accuracy: 0.868786\n",
      "Iteration: 540 Accuracy: 0.869524\n",
      "Iteration: 550 Accuracy: 0.870524\n",
      "Iteration: 560 Accuracy: 0.871286\n",
      "Iteration: 570 Accuracy: 0.872095\n",
      "Iteration: 580 Accuracy: 0.872833\n",
      "Iteration: 590 Accuracy: 0.873452\n",
      "Iteration: 600 Accuracy: 0.874357\n",
      "Iteration: 610 Accuracy: 0.874976\n",
      "Iteration: 620 Accuracy: 0.875643\n",
      "Iteration: 630 Accuracy: 0.876595\n",
      "Iteration: 640 Accuracy: 0.876929\n",
      "Iteration: 650 Accuracy: 0.877429\n",
      "Iteration: 660 Accuracy: 0.877786\n",
      "Iteration: 670 Accuracy: 0.878524\n",
      "Iteration: 680 Accuracy: 0.879381\n",
      "Iteration: 690 Accuracy: 0.880095\n",
      "Iteration: 700 Accuracy: 0.88069\n",
      "Iteration: 710 Accuracy: 0.881452\n",
      "Iteration: 720 Accuracy: 0.881905\n",
      "Iteration: 730 Accuracy: 0.882357\n",
      "Iteration: 740 Accuracy: 0.882786\n",
      "Iteration: 750 Accuracy: 0.883167\n",
      "Iteration: 760 Accuracy: 0.883643\n",
      "Iteration: 770 Accuracy: 0.883952\n",
      "Iteration: 780 Accuracy: 0.884571\n",
      "Iteration: 790 Accuracy: 0.885238\n",
      "Iteration: 800 Accuracy: 0.885786\n",
      "Iteration: 810 Accuracy: 0.88619\n",
      "Iteration: 820 Accuracy: 0.886643\n",
      "Iteration: 830 Accuracy: 0.887048\n",
      "Iteration: 840 Accuracy: 0.887595\n",
      "Iteration: 850 Accuracy: 0.888143\n",
      "Iteration: 860 Accuracy: 0.888333\n",
      "Iteration: 870 Accuracy: 0.888643\n",
      "Iteration: 880 Accuracy: 0.888952\n",
      "Iteration: 890 Accuracy: 0.889238\n",
      "Iteration: 900 Accuracy: 0.889405\n",
      "Iteration: 910 Accuracy: 0.889905\n",
      "Iteration: 920 Accuracy: 0.890262\n",
      "Iteration: 930 Accuracy: 0.890643\n",
      "Iteration: 940 Accuracy: 0.890976\n",
      "Iteration: 950 Accuracy: 0.891333\n",
      "Iteration: 960 Accuracy: 0.89169\n",
      "Iteration: 970 Accuracy: 0.892\n",
      "Iteration: 980 Accuracy: 0.892214\n",
      "Iteration: 990 Accuracy: 0.892524\n"
     ]
    }
   ],
   "source": [
    "L1, L2 = Train(Y, L0, L1, L2,  0.1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our model ###\n",
    "We can no see how accurate our model is on data it hasn't seen before\n",
    "\n",
    "Lets first import our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "handwritting_test_data_raw = pd.read_csv(\"data/Handwritting_dataset/test.csv\")\n",
    "testData = np.array(handwritting_test_data_raw).T / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write some methods to visualise the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePredictions(X,L1, L2):\n",
    "    Z1, A1, Z2, A2 = forwardPropagation(X, L1, L2)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def testPrediction(index,L1, L2):\n",
    "    current_image = testData[:, index, None]\n",
    "    prediction = makePredictions(testData[:, index, None], L1, L2)\n",
    "    print(\"Prediction: \", prediction)\n",
    "\n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation=\"nearest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def randomTest(n, L1, L2):\n",
    "    for i in range(n):\n",
    "        randomNum = random.randrange(0, testData[0].size)\n",
    "        testPrediction(randomNum,L1,L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "randomTest(3,L1,L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some notes ##\n",
    "The functions chosen above are arbitrary and you can swap them out and their equations.\n",
    "\n",
    "This is a simple neural network and there are multiple types of them like convolutional or radial basis, which have been optimised for specific problems, however the underlying concepts are the same.\n",
    "\n",
    "If the math wasn't comprehensive there are hundreds of tutorials that explain much better or with better mathematical rigour than this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net generalisation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronLayer:\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        self.weights = np.random.rand(outputSize, inputSize) - 0.5\n",
    "        self.bias = np.random.rand(outputSize, 1) - 0.5\n",
    "\n",
    "\n",
    "class NeuralNet:\n",
    "    L = []\n",
    "    numOutputs = 0\n",
    "    propagationSteps = 0\n",
    "\n",
    "    # Initialize\n",
    "    def __init__(self, input, numLayers, numNeuronsPerLayer):\n",
    "        self.L.append(input)\n",
    "        for i in range(numLayers):\n",
    "            if (i != 0) :\n",
    "                self.L.append(NeuronLayer(numNeuronsPerLayer[i-1], numNeuronsPerLayer[i]))\n",
    "\n",
    "        self.numOutputs = numNeuronsPerLayer[-1]\n",
    "        self.propagationSteps = numLayers - 1\n",
    "        print(self.propagationSteps, len(self.L))\n",
    "\n",
    "    # Forward propagation functions\n",
    "    def ReLU(self,Z):\n",
    "        return np.maximum(Z,0)\n",
    "\n",
    "    def softmax(self,Z):\n",
    "        A = np.exp(Z) / sum(np.exp(Z))\n",
    "        return A\n",
    "\n",
    "    def forwardPropagation(self):\n",
    "        L = self.L\n",
    "        pSteps = self.propagationSteps\n",
    "        Z = []\n",
    "        A = [self.L[0]]\n",
    "        for i in range(pSteps):\n",
    "            if i == pSteps:\n",
    "                Z.append(L[i + 1].weights.dot(A[i]) + L[i + 1].bias)\n",
    "                A.append(self.softmax(Z[i]))\n",
    "            Z.append(L[i+1].weights.dot(A[i]) + L[i+1].bias)\n",
    "            # print(Z[i].shape)\n",
    "            A.append(self.ReLU(Z[i]))\n",
    "\n",
    "        return A, Z\n",
    "\n",
    "    # Back propagation functions\n",
    "    def one_hot(self,Y):\n",
    "        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "        one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "        one_hot_Y = one_hot_Y.T\n",
    "        return one_hot_Y\n",
    "\n",
    "    def dReLU(self,Z):\n",
    "        return Z > 0\n",
    "\n",
    "    def gradients(self, A, Z, Y):\n",
    "        L = self.L\n",
    "        pSteps = self.propagationSteps\n",
    "        m = self.numOutputs\n",
    "        dZ = []\n",
    "        dW = []\n",
    "        dB = []\n",
    "        # print(pSteps)\n",
    "        hotY = self.one_hot(Y)\n",
    "        dZ.append((1/m) * (A[pSteps] - hotY))\n",
    "\n",
    "        for i in range(pSteps):\n",
    "            print(i)\n",
    "            dW.append(dZ[i].T.dot(A[pSteps - i]) ) \n",
    "            dB.append(np.sum(dZ[i]))\n",
    "            print(\n",
    "                L[pSteps - i].weights.shape,\n",
    "                dZ[i].shape,\n",
    "                Z[pSteps - i - 1].shape, \n",
    "        )\n",
    "            print(dW[i].shape, A[pSteps-i].T.shape)\n",
    "            if i < pSteps - 1:\n",
    "                dZ.append(L[pSteps - i].weights.T.dot(dZ[i]) * self.dReLU(Z[i]))\n",
    "\n",
    "        dZ = dZ[::-1]\n",
    "        dW = dW[::-1]\n",
    "        dB= dB[::-1]\n",
    "        print(len(dW), len(dB))\n",
    "        return dW, dB\n",
    "\n",
    "    def gradientDescent(self, dW, dB, alpha):\n",
    "        for i in range(self.propagationSteps - 1):\n",
    "            print(self.L[i + 1].weights.shape, dW[i].shape, self.L[i + 1].bias.shape, dB[i])\n",
    "            self.L[i+1].weights -= alpha * dW[i]\n",
    "            self.L[i+1].bias -= alpha * dB[i]\n",
    "\n",
    "    def backPropagation(self, Y, A, Z, alpha):\n",
    "        dW, dB = self.gradients(A, Z, Y)\n",
    "        self.gradientDescent(dW, dB, alpha)\n",
    "\n",
    "    def get_predictions(self,O):\n",
    "        return np.argmax(O, 0)\n",
    "\n",
    "    def get_accuracy(self,predictions, Y):\n",
    "        return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "    def Train(self, Y, alpha, iterations):\n",
    "        for i in range(iterations):\n",
    "            A, Z = self.forwardPropagation()\n",
    "            self.backPropagation(Y, A, Z, alpha)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                predictions = self.get_predictions(A[-1])\n",
    "                accuracy = self.get_accuracy(predictions, Y)\n",
    "\n",
    "                print( f\"Iteration:\", i, \"Accuracy:\", round(accuracy, 6) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "myN = NeuralNet(L0,3,[784, 16, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(10, 16) (10, 42000) (10, 42000)\n",
      "(42000, 42000) (42000, 10)\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmyN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 105\u001b[0m, in \u001b[0;36mNeuralNet.Train\u001b[0;34m(self, Y, alpha, iterations)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m    104\u001b[0m     A, Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforwardPropagation()\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackPropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    108\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_predictions(A[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[20], line 93\u001b[0m, in \u001b[0;36mNeuralNet.backPropagation\u001b[0;34m(self, Y, A, Z, alpha)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackPropagation\u001b[39m(\u001b[38;5;28mself\u001b[39m, Y, A, Z, alpha):\n\u001b[0;32m---> 93\u001b[0m     dW, dB \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradientDescent(dW, dB, alpha)\n",
      "Cell \u001b[0;32mIn[20], line 69\u001b[0m, in \u001b[0;36mNeuralNet.gradients\u001b[0;34m(self, A, Z, Y)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pSteps):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m---> 69\u001b[0m     dW\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdZ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpSteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m ) \n\u001b[1;32m     70\u001b[0m     dB\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39msum(dZ[i]))\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     72\u001b[0m         L[pSteps \u001b[38;5;241m-\u001b[39m i]\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m     73\u001b[0m         dZ[i]\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m     74\u001b[0m         Z[pSteps \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, \n\u001b[1;32m     75\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "myN.Train(Y,0.1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
