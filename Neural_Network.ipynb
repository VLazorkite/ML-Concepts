{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network #\n",
    "## Python Tutorial ##\n",
    "\n",
    "Neural networks are computational models inspired by the human brain's structure and function, capable of learning and making predictions from complex data.\n",
    "\n",
    "This looks something like this\n",
    "\n",
    "![SimpleNN](https://www.researchgate.net/publication/337469702/figure/fig1/AS:828416181932032@1574521217879/Simple-neural-network-diagram-http-cs231ngithubio-neuralnetworks-1-The-nodes-are.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks process input data through layers of neurons, where each neuron applies transformations to the data using activation functions and learns to make predictions by adjusting its parameters during training, aiming to minimize prediction errors.\n",
    "\n",
    "<i>Note: There are as many hidden layers and neurons as you want and more does not necessarily imply better</i>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries ####\n",
    "\n",
    "Here are pieces of other people's code that will help as understand what we're doing along the way\n",
    "\n",
    "To install these click terminal on whichever IDE you're using and use pip to instal eg: <code>pip install matplotlib.pyplot</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do It ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the MNIST dataset which is a dataset for training simple neural networks on handwritten digits\n",
    "\n",
    "We will train and run our model on subsets of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handwritting_data_raw = pd.read_csv(\"data/Handwritting_dataset/train.csv\")\n",
    "handwritting_data_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents the a handwritten digit in a 28 by 28 pixel grid and the value brigtness of each pixel.\n",
    "\n",
    "This is how a digit would look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1255ff490>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert the data to numpy arrays\n",
    "handwritting_data = np.array(handwritting_data_raw)\n",
    "\n",
    "exampleMatrix = handwritting_data[3][1:].reshape(28, 28)\n",
    "\n",
    "plt.imshow(exampleMatrix, cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation ##\n",
    "This is the passing of data from the input layer to the output layer through the hidden layers. Each neuron after the input to output layer has 2 components called the weights and bias. <b>Weights</b> determine the strength of influence that one neuron has with the next neuron it transmits information to. <b>Biases</b> are the initial value that a weight can possess even if all parameters are 0.\n",
    "\n",
    "The process of information transmission is done through matrix multiplication of the input and the weights and addition of biases of each neutron much resembling a linear function. The result of this is then applied through an activation function. \n",
    "\n",
    "The <b>activation function</b> is a function that changes the output from being linear which gives the neural network all its magic. This is like painting a picture but now from only using straight lines we can introduce curves and contrasts. The activation then refers to the final output value.\n",
    "\n",
    "![Yo](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*U3QZ_Yn4fcjbdUkIwHJ90w.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math ###\n",
    "<i>A basic understanding of matrices and matrix multiplication is needed<i>\n",
    "\n",
    "So a neuron receives input and applies a linear transformation to it and apply an activation function to it.\n",
    "\n",
    "$$ r = wx + b $$\n",
    "$$ a = f(z) $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$ w(weights) \\in \\mathbb{R}^{m\\times1} ,\\hspace{5mm} x(inputs) \\in \\mathbb{R}^{1\\times m} ,\\hspace{5mm} b(bias), \\hspace{1.5mm} z(linear \\hspace{1.5mm} result), \\hspace{2mm} a(activation) \\in  \\mathbb{R}^1$$\n",
    "\n",
    "and f is the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can scale this up to a whole layer of neurons so where for a neuron in layer i:\n",
    "\n",
    "$$ w^{[i]} = \\begin{bmatrix}w^{[i]}_1, w^{[i]}_2, & \\cdots &, w^{[i]}_n \\end{bmatrix}, \\hspace{5mm} x = \\begin{bmatrix}x^{[i]}_1, x^{[i]}_2, & \\cdots &, x^{[i]}_n \\end{bmatrix}^T \\hspace{3mm} and \\hspace{3mm} b^{[i]} = b^{[i]} $$\n",
    "\n",
    "The operations in the layer can be represented with the equations:\n",
    "\n",
    "$$ Z = WX + B$$\n",
    "$$ A = f(R) $$\n",
    "\n",
    "where\n",
    "$$ W^{[i]} = \\begin{bmatrix}w^{[i]}_{11} & w^{[i]}_{12} & \\cdots & w^{[i]}_{1n}\\\\ w^{[i]}_{21} & \\ddots& \\cdots & \\vdots \\\\ \\vdots & \\vdots& \\ddots & \\vdots\\\\ w^{[i]}_{m1} & \\cdots & \\cdots & w^{[i]}_{mn}\\\\ \\end{bmatrix}, \\hspace{5mm} X^{[i]} = \\begin{bmatrix}x^{[i]}_{11} & x^{[i]}_{12} & \\cdots & x^{[i]}_{1n}\\\\ x^{[i]}_{21} & \\ddots& \\cdots & \\vdots \\\\ \\vdots & \\vdots& \\ddots & \\vdots\\\\ x^{[i]}_{m1} & \\cdots & \\cdots & x^{[i]}_{mn}\\\\ \\end{bmatrix}^T \\hspace{3mm} and \\hspace{3mm} B^{[i]} = \\begin{bmatrix}b^{[i]}_1 \\\\ b^{[i]}_2 \\\\ \\vdots \\\\ b^{[i]}_m\\end{bmatrix} $$ \n",
    "\n",
    "This works because matrices can be layered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for our neural network we will have 784 neurons in the input layers for the 28 by 28 pixel grid and I chose 1 hidden layer with 16 neurons and lastly the output layer will have 10, corresponding to the 10 digits it could be.\n",
    "\n",
    "<i>Note: The input layer does not have any weights and biases because it introduces primary data.</i>\n",
    "\n",
    "We first want to normalise the pixel data as each value is given from 0 to 255 which is computationally complex whereas working from 0 to 1 is much better.\n",
    "\n",
    "When initialising The values for weights and biases can just be randomised as the values are updated after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LayerInit(numNeurons, numPreviousLayerNeurons):\n",
    "    # creates a randomised matrix\n",
    "    weights = np.random.rand(numNeurons, numPreviousLayerNeurons) - 0.5\n",
    "    bias = np.random.rand(numNeurons, 1) - 0.5\n",
    "    return {\"weights\": weights, \"bias\": bias}\n",
    "\n",
    "\n",
    "L0 = handwritting_data[1:]/255\n",
    "L1 = LayerInit(16, 784)\n",
    "L2 = LayerInit(10, 16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our activation function we have some guidelines to choosing one that works.\n",
    "<ul>\n",
    "<li><b>Non-linear</b> function, as mentioned earlier to have complexity</li>\n",
    "<li>It has to be <b>differentiable</b> this is to enable optimization which will be seen later when training</li>\n",
    "<li>It should have <b>appropriate range</b>. For classifying multiple classes some functions produce probability functions and for binary classification functions with a smaller range work well</li>\n",
    "<li><b>Computational complexity</b> should be low and effecient to evaluate especially as training large models takes significant resources</li>\n",
    "</ul>\n",
    "\n",
    "Common activation functions are:\n",
    "\n",
    "\n",
    "![activationfunctions](https://lh4.googleusercontent.com/hTeaMXYrsBlpKrGvRCvSX8maYuU4Zhd9-6B_Z3QjnnpE02MhfFK8IHgrDsX9U9SoSw9MIJFQbQyR64PHqNjGfMa8LgUctX5ht0Z21NxqJ-AAd5bU30mFGaTzNhiNuiwO2OVvpfYYFAonf3k8wQTqwGA)\n",
    "\n",
    "The functions we use are : [ReLU]('https://www.nomidl.com/wp-content/uploads/2022/04/image-10.png') which returns values between 0 and 1(We normalised the data) for the hidden layer and [Softmax]('https://siegel.work/blog/ActivationFunctions/img/softmax.png') which returns exaggarated probability values for the output layer\n",
    "\n",
    "$$ ReLU(Z) = max (0,Z) $$\n",
    "$$softmax(Z) = \\frac{e^{z_i}}{\\sum_{j=1}^{N} e^{z_j}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z,0)\n",
    "\n",
    "def softmax(Z):\n",
    "    O = np.exp(Z) / sum(np.exp(Z))\n",
    "    return O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly lets put all these steps together to create a function called foward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(input,hiddenlayer,output):\n",
    "    Z1 = hiddenlayer['weights'].dot(input) + hiddenlayer['bias']\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = output['weights'].dot(A1) + output['bias']\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flow ####\n",
    "\n",
    "$$ \\rightarrow Z^{[1]} = W^{[1]}X^{[0]} + B^{[1]} $$ \n",
    "$$ \\rightarrow A^{[1]} = f_{ReLU}(Z^{[1]}) $$\n",
    "$$ \\rightarrow Z^{[2]} = W^{[2]}A^{[1]} + B^{[2]} $$\n",
    "$$ \\rightarrow A^{[2]} = g_{softmax}(Z^{[2]}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation ##\n",
    "So how does the neural network learn?\n",
    "\n",
    "Great question! This is called backpropagation which is the key algorithm for training our model. As you can see we randomised our weights and biases when initialising, this is because adjusting each weight and bias manually would take ages.\n",
    "\n",
    "We train our model by letting our model guess and then telling it how far it was from the correct answer to update its parameters. Your goal is to minimize the distance to the correct answer. Think of training a neural network like navigating through a maze to reach a treasure. The cost function is like a guide telling you how far you are from the treasure after each step you take. Your goal is to minimize this distance to get as close to the treasure as possible.\n",
    "\n",
    "We call the distance to treasure the <b>Loss function</b>. We then want to change the weights and biases in each layer to minimize this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math ###\n",
    "<i>The tricky bit. It's long and gruesome and you can colapse it if you'd like, but it is the derivation of possibly the coolest code we have ever made</i>\n",
    "\n",
    "A decent understanding of calculus is needed but the steps we take are:\n",
    "<ul>\n",
    "<li>Chose a loss function</li>\n",
    "<li>Minimize it with respect to weights and biases</li>\n",
    "<li>Gradient descent. These are the steps that vary the weights and biases to most effeciently decrease the loss function</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivations ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to chose a loss function. To keep it simple we will use the Mean Squared Error as our loss.\n",
    "$$ L = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2 $$\n",
    "$$ L = \\text{loss} \\hspace{5mm} m = \\text{number of output neurons} \\hspace{5mm} y_i = \\text{correct probability} \\hspace{5mm} \\hat{y_i} = \\text{predicted probability} $$\n",
    "\n",
    "Now we want want thisloss to decrease most for the smallest change in the weights and biases. As usual with minimization problems we need calculus. \n",
    "\n",
    "Differenciate the loss with respect to the predictions  \n",
    "\n",
    "$$ \\frac{dL}{d\\hat{y}}= \\frac{2}{m}\\sum_{i=1}^{m} (\\hat{y}_i - y_i)$$\n",
    "\n",
    "We can represent this with our matrix notation from before\n",
    "$$ \\frac{dL}{dA^{[2]}} = \\frac{2}{m}(A^{[2]} - Y) $$\n",
    "\n",
    "As\n",
    "$$ \\hat{y} = A^{[2]} \\hspace{5mm} and \\hspace{5mm} y = Y $$\n",
    "\n",
    "We then want to then find the the minimum with respect to the weight and biases. \n",
    "\n",
    "To get the minimum of the loss with respect to the weights we need to calculate the partial deriavtive. Which we can use the chain rule for\n",
    "$$ \\frac{\\partial L}{\\partial W^{[2]}} = \\frac{\\partial L}{\\partial A^{[2]}} \\times \\frac{\\partial A^{[2]}}{\\partial W^{[2]}} $$\n",
    "\n",
    "Since \n",
    "$$ A^{[2]} = f(W^{[2]}A^{[1]} + B^{[2]})$$\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W^{[2]}} = \\frac{2(A^{[2]} - Y)}{m}\\times A^{[1]}f'(W^{[2]}A^{[1]} + B^{[2]})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final equations ###\n",
    "\n",
    "Derivatives\n",
    "$$ dZ^{[2]} = A^{[2]} - Y $$\n",
    "$$ dW^{[2]} = \\frac{1}{m}Z^{[2]}A^{[2]\\hspace{1mm}T} $$\n",
    "$$ dB^{[2]} = \\frac{1}{m} \\sum{dZ^{[2]}}$$\n",
    "$$ dZ^{[1]} = W^{[2]T} dZ^{[2]} \\times g^{[1]\\prime} (z^{[1]})$$\n",
    "$$ dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T}$$\n",
    "$$ dB^{[1]} = \\frac{1}{m} \\sum{dZ^{[1]}}$$\n",
    "\n",
    "Changes\n",
    "$$ W^{[2]} := W^{[2]} - \\alpha dW^{[2]}$$\n",
    "$$ b^{[2]} := b^{[2]} - \\alpha db^{[2]}$$\n",
    "$$ W^{[1]} := W^{[1]} - \\alpha dW^{[1]}$$\n",
    "$$ b^{[1]} := b^{[1]} - \\alpha db^{[1]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
